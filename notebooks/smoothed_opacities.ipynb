{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothed Opacities\n",
    "\n",
    "In this notebook we will smooth single grain opacities over a small distribution of similar sized particles to avoid strong, unrealistic resonances.\n",
    "\n",
    "Since linear averaging produces smoother results, we will use those as standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dsharp_opac as opacity\n",
    "\n",
    "import __main__ as main\n",
    "interactive = not hasattr(main, '__file__')\n",
    "if interactive:\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().magic('matplotlib inline')\n",
    "    \n",
    "plt.style.use([{'figure.dpi':150}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the size and wavelength grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.logspace(-5, 2, 200)\n",
    "lam = np.logspace(np.log10(1e-5), 1, 210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set wheter large grains should be extrapolated (large speed-up, but less precise) or not (very slow, but more precise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extrapolate_large_grains = False\n",
    "fm_ice = 0.2\n",
    "output_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fm_ice == 0.2:\n",
    "    name = 'default_opacities'\n",
    "elif fm_ice == 0.0:\n",
    "    name = 'icefree_opacities'\n",
    "else:\n",
    "    warnings.warn('fm_ice should be 0.0 or 0.2')\n",
    "    name = 'other_opacities'\n",
    "    \n",
    "if extrapolate_large_grains:\n",
    "    name += '_extrapol'\n",
    "    \n",
    "print('opacity name: {}'.format(name))\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the current default opacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc,rho_s = opacity.get_dsharp_mix(fm_ice=fm_ice)\n",
    "res_default = opacity.get_opacities(a, lam, rho_s=rho_s, diel_const=dc, extrapolate_large_grains=extrapolate_large_grains)\n",
    "\n",
    "k_abs  = res_default['k_abs']\n",
    "k_sca  = res_default['k_sca']\n",
    "g      = res_default['g']\n",
    "theta  = res_default['theta']\n",
    "S1     = res_default['S1']\n",
    "S2     = res_default['S2']\n",
    "rho_s  = res_default['rho_s']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the default opacities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    os.path.join(output_dir,name),\n",
    "    a     = res_default['a'],\n",
    "    lam   = res_default['lam'],\n",
    "    k_abs = res_default['k_abs'],\n",
    "    k_sca = res_default['k_sca'],\n",
    "    g     = res_default['g'],\n",
    "    rho_s = res_default['rho_s'],\n",
    "    S1    = res_default['S1'],\n",
    "    S2    = res_default['S2'],\n",
    "    theta = res_default['theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(os.path.join(output_dir,name + '.npz')) as data:\n",
    "    a     = data['a']\n",
    "    lam   = data['lam']\n",
    "    k_abs = data['k_abs']\n",
    "    k_sca = data['k_sca']\n",
    "    g     = data['g']\n",
    "    rho_s = data['rho_s']\n",
    "    S1    = res_default['S1']\n",
    "    S2    = res_default['S2']\n",
    "    theta = res_default['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_l = opacity.get_smooth_opacities(a, lam, rho_s, dc, smoothing='linear', extrapolate_large_grains=extrapolate_large_grains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_l.pickle', 'wb') as fid:\n",
    "    pickle.dump(res_l, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_l.pickle', 'rb') as fid:\n",
    "    res_l = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_l_h     = res_l['a_h']\n",
    "\n",
    "k_abs_l_h = res_l['k_abs_h']\n",
    "k_sca_l_h = res_l['k_sca_h']\n",
    "g_l_h     = res_l['g_h']\n",
    "S1_l_h    = res_l['S1_h']\n",
    "S2_l_h    = res_l['S2_h']\n",
    "\n",
    "k_abs_l = res_l['k_abs']\n",
    "k_sca_l = res_l['k_sca']\n",
    "g_l     = res_l['g']\n",
    "S1_l    = res_l['S1']\n",
    "S2_l    = res_l['S2']\n",
    "\n",
    "theta     = res_l['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(output_dir,name + '_smooth'),  a=a,     lam=lam, k_abs=k_abs_l,   k_sca=k_sca_l,   g=g_l,   rho_s=rho_s, S1=S1_l,   S2=S2_l,   theta=theta)\n",
    "np.savez_compressed(os.path.join(output_dir,name + '_highres'), a=a_l_h, lam=lam, k_abs=k_abs_l_h, k_sca=k_sca_l_h, g=g_l_h, rho_s=rho_s, S1=S1_l_h, S2=S2_l_h, theta=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first part of the grid to make sure it looks correct (no double points and such)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inter = 40\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True)\n",
    "ax[0].plot(a[0:2], np.ones(2), 'x')\n",
    "ax[0].plot(a_l_h[0:n_inter], np.ones(n_inter), 'r+')\n",
    "ax[0].plot(a_l_h[n_inter:2 * n_inter], np.ones(n_inter), 'g+')\n",
    "ax[0].set_xlim(a[0] - 1.1 * 0.5 * (a[1] - a[0]), a[1])\n",
    "\n",
    "ax[1].plot(np.arange(0,n_inter), a_l_h[0:n_inter], 'x');\n",
    "ax[1].plot(np.arange(n_inter,2 * n_inter), a_l_h[n_inter:2 * n_inter], 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_g = opacity.get_smooth_opacities(a, lam, rho_s, dc, smoothing='gaussian', extrapolate_large_grains=extrapolate_large_grains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_g.pickle', 'wb') as fid:\n",
    "    pickle.dump(res_g, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_g.pickle', 'rb') as fid:\n",
    "    res_g = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_g_h     = res_g['a_h']\n",
    "\n",
    "k_abs_g_h = res_g['k_abs_h']\n",
    "k_sca_g_h = res_g['k_sca_h']\n",
    "g_g_h     = res_g['g_h']\n",
    "S1_g_h    = res_g['S1_h']\n",
    "S2_g_h    = res_g['S2_h']\n",
    "\n",
    "k_abs_g = res_g['k_abs']\n",
    "k_sca_g = res_g['k_sca']\n",
    "g_g     = res_g['g']\n",
    "S1_g    = res_g['S1']\n",
    "S2_g    = res_g['S2']\n",
    "\n",
    "theta   = res_g['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first part of the grid to make sure it looks correct (no double points and such)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inter = 40\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 3), tight_layout=True)\n",
    "ax[0].plot(a[0:3], np.ones(3), 'x', markersize=10)\n",
    "for i in range(3):\n",
    "    ax[0].semilogx(a_g_h[i * n_inter: (i + 1) * n_inter], np.ones(n_inter), '+')\n",
    "ax[0].set_xlim(a[0] - 1.1 * 0.5 * (a[1] - a[0]), a[i] + 0.5 * (a[i + 1] - a[i]))\n",
    "\n",
    "ax[1].plot(np.arange(0,n_inter),a_g_h[0:n_inter], 'x');\n",
    "ax[1].plot(np.arange(n_inter,2 * n_inter), a_g_h[n_inter:2 * n_inter], 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the size averaged opacity and $\\beta$ as function of $a_\\mathrm{max}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_avg =[0.1, 0.3]\n",
    "\n",
    "f, axs = plt.subplots(2,1,sharex=True, figsize=(6,6))\n",
    "\n",
    "r = opacity.size_average_opacity(lam_avg, a, lam, k_abs, k_sca, plot=True, ax=axs)\n",
    "for i, _lam in enumerate(lam_avg):\n",
    "    print('kappa_abs @ {:7.3g} mm for a_max = 1mm : {:.3g} cm^2/g'.format(_lam *10, np.interp(_lam, a, r['ka'][i])))\n",
    "\n",
    "rl = opacity.size_average_opacity(lam_avg, a, lam, k_abs_l, k_sca_l, plot=False)\n",
    "r['ax2'].semilogx(a, rl['beta'],'--',label='linear')\n",
    "\n",
    "rg = opacity.size_average_opacity(lam_avg, a, lam, k_abs_g, k_sca_g, plot=False)\n",
    "r['ax2'].semilogx(a, rg['beta'],'-.',label='Gaussian')\n",
    "r['ax2'].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the opacities averaged over a typical size distribution to check if the results got smoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 1g-normalized size distribution (bin-integrated) up to 1 mm\n",
    "\n",
    "s = a**0.5\n",
    "s[a > 0.1] = 0\n",
    "s= s / s.sum()\n",
    "\n",
    "# size average the absorption opacities\n",
    "\n",
    "ka0 = (k_abs * s[:,None]).sum(0)\n",
    "kal = (k_abs_l * s[:,None]).sum(0)\n",
    "kag = (k_abs_g * s[:,None]).sum(0)\n",
    "\n",
    "\n",
    "# where to measure the reference value\n",
    "lam_obs = 0.087\n",
    "\n",
    "# load the D'Alessio opacity\n",
    "d2g = sum([0.0056, 0.0034, 0.000768, 0.0041])\n",
    "data_d01 = np.loadtxt(opacity.get_datafile('kappa_D01_T100K_p3.5_amax1mm.csv'))\n",
    "lam_d01 = 10.**data_d01[:,0] * 1e-4\n",
    "kap_d01 = 10.**(data_d01[:,1]) / d2g\n",
    "\n",
    "# the Beckwith 1990 law\n",
    "\n",
    "kb = 3.5 * (lam / 0.087)**(-1)  # Beckwith 1990\n",
    "\n",
    "# the opacities from Andrews et al. 2009\n",
    "\n",
    "la, ka = np.loadtxt(opacity.get_datafile('andrews2009.dat')).T\n",
    "\n",
    "# now the plot\n",
    "\n",
    "f, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.plot(np.log10(1e4*lam),     np.log10(kb),      'k--', zorder=-100, alpha=0.5, label='Beckwith et al. 1990, $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(     lam_obs*1e4, np.interp(lam_obs,lam,kb)))\n",
    "ax.plot(np.log10(la),          np.log10(ka),      'k--', zorder=-100, alpha=1.0, label='Andrews et al. 2009, $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(      lam_obs*1e4, np.interp(lam_obs*1e4,la,ka)))\n",
    "ax.plot(np.log10(1e4*lam_d01), np.log10(kap_d01), 'k:',  zorder=-100, alpha=1.0, label='D\\'Alessio et al. 2001, $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(   lam_obs*1e4, np.interp(lam_obs,lam_d01,kap_d01)))\n",
    "ax.plot(np.log10(1e4*lam),     np.log10(ka0),            zorder=-100, lw=1, alpha=1.0, label='DSHARP (non-avg), $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(         lam_obs*1e4, np.interp(lam_obs,lam,ka0)))\n",
    "ax.plot(np.log10(1e4*lam),     np.log10(kal),            zorder=-100, lw=1, alpha=1.0, label='DSHARP (linear-avg), $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(      lam_obs*1e4, np.interp(lam_obs,lam,kal)))\n",
    "ax.plot(np.log10(1e4*lam),     np.log10(kag),            zorder=-100, lw=1, alpha=1.0, label='DSHARP (gauss-avg), $\\kappa_{{{:.0f}}} = {:3.2g}$ cm$^2$/g'.format(       lam_obs*1e4, np.interp(lam_obs,lam,kag)))\n",
    "\n",
    "ax.legend(loc=3, fontsize='small')\n",
    "ax.set_xlim(-1,4)\n",
    "ax.set_ylim(-1,5)\n",
    "ax.set_xlabel('log $\\lambda$ [$\\mu$m]')\n",
    "ax.set_ylabel('log $\\kappa$ (dust) [cm$^2$/g]')\n",
    "plt.savefig('avgs.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a movie scrolling through all wavelengths to show how much the fits differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviename = 'movie01.mp4'\n",
    "\n",
    "if not os.path.isdir('imgs'):\n",
    "    os.mkdir('imgs')\n",
    "\n",
    "if os.path.isfile(moviename):\n",
    "    os.unlink(moviename)\n",
    "    \n",
    "with plt.rc_context(rc={'lines.linewidth': 1}):\n",
    "    f, axs = plt.subplots(2, 1, figsize=(15,10), sharex=True)\n",
    "    \n",
    "    for ilam in range(len(lam)):\n",
    "        if ilam==0:\n",
    "            l01, = axs[0].loglog(a, k_abs[:, ilam], 'k', label='original')\n",
    "            l02, = axs[0].loglog(a_l_h, k_abs_l_h[:, ilam],'-', lw=1, c='0.75', label='highres')\n",
    "            l03, = axs[0].loglog(a, k_abs_l[:, ilam], label='linear-averaged')\n",
    "            l04, = axs[0].loglog(a, k_abs_g[:, ilam], label='gauss-averaged')\n",
    "            leg=axs[0].legend()\n",
    "            leg.set_title('absorption opacity', {'size':'large'})\n",
    "\n",
    "            l11, = axs[1].loglog(a, k_sca[:, ilam], 'k', label='original')\n",
    "            l12, = axs[1].loglog(a_l_h, k_sca_l_h[:, ilam],'-', lw=1, c='0.75', label='highres')\n",
    "            l13, = axs[1].loglog(a, k_sca_l[:, ilam], label='linear-averaged')\n",
    "            l14, = axs[1].loglog(a, k_sca_g[:, ilam], label='gauss-averaged')\n",
    "            leg = axs[1].legend()\n",
    "            leg.set_title('scattering opacity', {'size':'large'})\n",
    "\n",
    "            for ax in axs:\n",
    "                ax.set_xlim(1e-4,3e0)\n",
    "                ax.set_ylim(ymin=2e-3, ymax=1e3)\n",
    "                ax.set_xlabel('particle radius [cm]')\n",
    "\n",
    "            axs[0].set_ylabel('absorption opacity [cm]')\n",
    "            axs[1].set_ylabel('scattering opacity [cm]')\n",
    "            ti = axs[0].set_title(f'wave length = {lam[ilam]:3.2} cm')\n",
    "            f.subplots_adjust(hspace=0)\n",
    "        else:\n",
    "            l01.set_ydata(k_abs[:, ilam])\n",
    "            l02.set_ydata(k_abs_l_h[:, ilam])\n",
    "            l03.set_ydata(k_abs_l[:, ilam])\n",
    "            l04.set_ydata(k_abs_g[:, ilam])\n",
    "            \n",
    "            l11.set_ydata(k_sca[:, ilam])\n",
    "            l12.set_ydata(k_sca_l_h[:, ilam])\n",
    "            l13.set_ydata(k_sca_l[:, ilam])\n",
    "            l14.set_ydata(k_sca_g[:, ilam])\n",
    "            \n",
    "            ti.set_text(f'wave length = {lam[ilam]:3.2} cm')\n",
    "        f.savefig(f'imgs/img_{ilam:03d}.png',dpi=200)\n",
    "\n",
    "if os.system('ffmpeg -r 60 -f image2 -i imgs/img_%03d.png -vcodec libx264 -crf 25 -pix_fmt yuv420p ' + moviename) == 0:\n",
    "    shutil.rmtree('imgs')\n",
    "    print('finished making movie')\n",
    "else:\n",
    "    if shutil.which('ffmpeg') is None:\n",
    "        print('ffmpeg not available, keeping the frames in folder \\'img\\'')\n",
    "    else:    \n",
    "        print('making movie failed, will not delete the frames')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
